\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{listings}
\usepackage{url}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{underscore}
\usepackage{float}
\lstset{%
  basicstyle=\small\ttfamily,
  mathescape=true,
  upquote=true,
}

\usepackage{amsmath,amsbsy,amsfonts,amssymb,amsthm,color,dsfont,mleftright,commath}

\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}

% \bbA, \bbB, ...
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \cA, \cB, ...
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \vA, \vB, ..., \va, \vb, ...
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop

% \valpha, \vbeta, ...,  \vGamma, \vDelta, ...,
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{\csname #1\endcsname}}}}
\ddefloop {alpha}{beta}{gamma}{delta}{epsilon}{varepsilon}{zeta}{eta}{theta}{vartheta}{iota}{kappa}{lambda}{mu}{nu}{xi}{pi}{varpi}{rho}{varrho}{sigma}{varsigma}{tau}{upsilon}{phi}{varphi}{chi}{psi}{omega}{Gamma}{Delta}{Theta}{Lambda}{Xi}{Pi}{Sigma}{varSigma}{Upsilon}{Phi}{Psi}{Omega}{ell}\ddefloop

\newcommand\braces[1]{\{#1\}}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newenvironment{solution}{\noindent\emph{Solution.}}{\hfill$\square$}

%-------------------------------------------------------------------------------

\title{COMS 4771 Spring 2017 Homework 4}
\author{Jin Tack Lim, jl4312
  }
\date{%
  }

\begin{document}
\maketitle


%-------------------------------------------------------------------------------

Problem 1.

(a) 

Consider the case in Figure~\ref{fig:one-rest}. There are three cases: C1, C2 and C3. Using
'one vs. rest', we can draw three lines, which classify a sample to either a
specific class or other classes. The whole space is divided into seven regions.
Out of seven, there are three regions that is classified to a specific class
clearly: R2 is C2, R4 is C3 and R6 is C1.  However there is a region that
doesn't fit into any class: R1. Also, there are regions that are classified
into multiple classes: R3, R5, R7.

So, this example showed that 'one vs. rest' has some drawbacks.

\begin{figure}[h]
  \centering
  \includegraphics[width=10cm]{one-rest}
  \caption{One vs. rest}
  \label{fig:one-rest}
\end{figure}

\pagebreak

(b)

Consider the case in Figure~\ref{fig:one-one}. There are three cases: C1, C2 and C3. Using
'one vs. one', we can draw three lines (3*2/2), which classify a sample to one
of two classes for each pair of classes.  The whole space is divided into seven
regions.  Out of seven, six regions are clearly classified into a specific
class: R6 and R7 are C1, R2 and R3 are C2, R4 and R5 are C3.  However R1 can't
be classified because there is no single winner: 1 vote for each classes.

So, this example showed that 'one vs. one' also has some drawbacks.

\begin{figure}[h]
  \centering
  \includegraphics[width=10cm]{one-one}
  \caption{One vs. One}
  \label{fig:one-one}
\end{figure}

\pagebreak

(c)

1. The primal and quadratic problem:

\begin{equation*}
\arg \min_{w,b} \frac{1}{2} \sum_{m=1}^{k} \norm{w}^2 + C \sum_{i=1}^{N}\xi_i \quad \text{such that}
\end{equation*}
\begin{equation*}
\quad y_i (w_m^T x_i + b) -1 + \xi_i \ge 0 \quad \text{for each dimension m (from 1 to k)}
\end{equation*}

With Lagrange $\alpha$:

\begin{equation*}
\arg \min_{w,b} \frac{1}{2} \sum_{m=1}^{k} \norm{w}^2 + C \sum_{i=1}^{N}\xi_i
- \sum_{i=1}^{N}\sum_{m=1}^{k}\alpha_{i}^{m}(y_i(w_m^T x_i + b) -1 + \xi_i)
- \sum_{i=1}^{N}\beta_i\xi_i
\end{equation*}
all $\alpha$ and $\beta \ge 0$

\bigskip


2. The dual problem:
%We first define a new notation $A_i$
%\begin{equation*}
%A_i = \sum_{m=1}^{k} \alpha_{i}^{m}
%\end{equation*}

\begin{equation*}
\arg \max_\alpha \sum_{i,m} \alpha_{i}^{m} - \frac{1}{2} \sum_{i,j,m} \alpha_{i}^{m} \alpha_{j}^{m} y_i y_j x_i^T x_j
\end{equation*}
\begin{equation*}
\text{subject to} \quad 0 \le \sum_{m}\alpha_{i}^{m} \le C \quad \text{and} \sum_{i=1}^{N}\sum_{m=1}^{k} \alpha_{i}^{m} y_i =0 
\end{equation*}

\pagebreak

Problem 2.

\bigskip

(a)
\begin{equation*}
\sum_{i=1}^{k}p_i \log p_i
\end{equation*}


This is the Entropy without the negative sign.

This is how I got the expectation.

(1) When the value of $y_i$ is $j$, then its contribution is $\log p_j$.

(2) The probability that $y_i$ becomes $j$ is $p_j$.

(3) Therefore the expectation is the sum of $p_j \log p_j$ over all possible j (1,...,k)

\bigskip
(b)

\begin{equation*}
 \frac{\sum_{i=1}^{k} m_i \log q_i}{N} 
\end{equation*}

 where $m_i$ represents the number of $i$ class samples in $Y$

\bigskip

Rationale: Now that we know the real probabilities and the real assignments for each $y_i$ in Y, we can compute the log-likelihood, which is

$ \sum_{i=1}^{k} m_i \log q_i $ where $m_i$ represents the number of $i$ class samples in $Y$

We have N elements in $Y$, so the expection of each $y_i$ would be 

$ \frac{\sum_{i=1}^{k} m_i \log q_i}{N} $
\end{document}

