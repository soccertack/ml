\documentclass[11pt]{article}
\usepackage[letterpaper,margin=1in]{geometry}
\usepackage{listings}
\usepackage{url}
\usepackage{hyperref}
\usepackage{textcomp}
\usepackage{graphicx}
\usepackage{underscore}
\usepackage{float}
\lstset{%
  basicstyle=\small\ttfamily,
  mathescape=true,
  upquote=true,
}

\usepackage{amsmath,amsbsy,amsfonts,amssymb,amsthm,color,dsfont,mleftright,commath}

\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}

% \bbA, \bbB, ...
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \cA, \cB, ...
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \vA, \vB, ..., \va, \vb, ...
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop

% \valpha, \vbeta, ...,  \vGamma, \vDelta, ...,
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{\csname #1\endcsname}}}}
\ddefloop {alpha}{beta}{gamma}{delta}{epsilon}{varepsilon}{zeta}{eta}{theta}{vartheta}{iota}{kappa}{lambda}{mu}{nu}{xi}{pi}{varpi}{rho}{varrho}{sigma}{varsigma}{tau}{upsilon}{phi}{varphi}{chi}{psi}{omega}{Gamma}{Delta}{Theta}{Lambda}{Xi}{Pi}{Sigma}{varSigma}{Upsilon}{Phi}{Psi}{Omega}{ell}\ddefloop

\newcommand\braces[1]{\{#1\}}

\theoremstyle{definition}
\newtheorem{problem}{Problem}
\newenvironment{solution}{\noindent\emph{Solution.}}{\hfill$\square$}

%-------------------------------------------------------------------------------

\title{COMS 4771 Spring 2017 Homework 5}
\author{Jin Tack Lim, jl4312
  }
\date{%
  }

\begin{document}
\maketitle


%-------------------------------------------------------------------------------

Problem 1.

(a) 

\begin{itemize}
\item Initialization

random $p^j(1), ..., p^j(D)$ where j = {1, ..., K}

$\pi_j = \frac{1}{K}$

\item Expectation

The probability that n-th document belongs to the class i.
For t-th iteration, the superscript t for each of $\tau, \pi, p^j(1), ..., p^j(D)$ are omitted.
\begin{equation*}
\tau_{n,i} = \frac{\pi_i \Pi_{d=1}^{D} p^i(d)^{x_n(d)}}{\sum_{j=1}^{K} \pi_j \Pi_{d=1}^{D} p^j(d)^{x_n(d)}}
\end{equation*}

\item Maximization
Update $p^i(1), ..., p^i(D)$ and $\pi_i$ where i = {1, ..., K}

\begin{equation*}
\pi_i^{(t+1)} = \frac{\sum_{q=1}^n \tau^{(t)}_{q, i}}{n}
\end{equation*}

\begin{equation*}
p^{i(t+1)}(d) = \frac{\sum_{q=1}^n \tau_{i,q} x(d)}{\sum_{q=1}^n \tau_{i,q} l_q}
\end{equation*}
where d={1, ..., D}, q = {1, ..., n}
\end{itemize}

(b)

The marginal distribution of $x_n$ is

\begin{equation*}
\frac{\hat{x_n}}{l_n}
\end{equation*}

This can be interpreted as the binomial distribution for each dimensions. So if
$l_n$ is very large and $\frac{\hat{x_n}}{l_n}$ is very small, it can be
approximated to the Poisson distribution, where $\lambda = \hat{x_n}$.

(c)
\begin{enumerate}
\item Similarity
\begin{itemize}
\item Both of them can reduce the dimensionality.
\end{itemize}

\item Difference
\begin{itemize}
\item PCA gives vectors in order such that the variance along the vector is high.
(e.g. the given data has the highest variance along the first vector.)
For (b), there's no guarantee.

\item PCA gives vectors that are orthogonal to each other, since they are all eigenvectors.
For (b), there's no guarantee.

\end{itemize}
\end{enumerate}

(d)

\pagebreak

Problem 2.

\bigskip

\pagebreak

Problem 3.

We have q+2 layers except the convolution layer. Input to each layer is $a^p$ and output is $z^p$ where p = 1, ..., q+2.

\begin{equation*}
R = \frac{1}{N} \sum_{n=1}^{N} \frac{1}{2} (y_n - z_n^{q+2})^2
\end{equation*}

\begin{enumerate}
\item the update equation for V

\begin{equation*}
\begin{split}
\frac{\partial R}{\partial V} & = \frac{\partial R}{\partial z^{q+2}}
				\frac{\partial z^{q+2}} {\partial a^{q+2}}
				\frac{\partial a^{q+2}} {\partial V} \\
				& = - \frac{1}{N} \sum_{n=1}^{N} (y - z^{q+2})
					z^{q+2}(1-z^{q+2}) z^{q+1}
\end{split}
\end{equation*}

Therefore, the update equation for V is
\begin{equation*}
V^{t+1} = V^t - \eta \frac{\partial R}{\partial V}
\end{equation*}

\item the update equation for U

\begin{equation*}
\begin{split}
\frac{\partial R}{\partial U} & = \frac{\partial R}{\partial z^{q+1}}
				\frac{\partial z^{q+1}} {\partial a^{q+1}}
				\frac{\partial a^{q+1}} {\partial U} \\
				& = \frac{\partial R}{\partial a^{q+2}}
				\frac{\partial a^{q+2}} {\partial z^{q+1}}
				\frac{\partial z^{q+1}} {\partial a^{q+1}}
				\frac{\partial a^{q+1}} {\partial U} \\
				& = \bigg( - \frac{1}{N} \sum_{n=1}^{N} (y - z^{q+2})
					z^{q+2}(1-z^{q+2}) \bigg)
					V
					z^{q+1}(1-z^{q+1}) z^q
\end{split}
\end{equation*}

The term put in the bracket is the one we can reuse from the previous derivation. (Backprop)

Therefore, the update equation for U is
\begin{equation*}
U^{t+1} = U^t - \eta \frac{\partial R}{\partial U}
\end{equation*}


\item the update equation for W

This is only valid for the points X which are selected in the down-sampling
layers. Since the down-sampling layers except the first one don't have weights,
we get the derivatve in the first down sampling layer.

\begin{equation*}
\begin{split}
\frac{\partial R}{\partial W} & = (\frac{\partial R}{\partial z^1})
				\frac{\partial z^1} {\partial a^1}
				\frac{\partial a^1} {\partial W} \\
				& = (\frac{\partial R}{\partial a^{2}}
				\frac{\partial a^{2}} {\partial z^{1}})
				\frac{\partial z^1} {\partial a^1}
				\frac{\partial a^1} {\partial W} \\
				& = (\frac{\partial R}{\partial a^{2}})
				\frac{\partial z^1} {\partial a^1}
				\frac{\partial a^1} {\partial W} \text{(------ *note 1)}\\
				& = (\frac{\partial R}{\partial a^{q+1}})
				\frac{\partial z^1} {\partial a^1}
				\frac{\partial a^1} {\partial W} \text{(------ *note 2)}\\
				& = \bigg( - \frac{1}{N} \sum_{n=1}^{N} (y - z^{q+2})
					z^{q+2}(1-z^{q+2})
					V
					z^{q+1}(1-z^{q+1}) \bigg)
				\frac{\partial z^1} {\partial a^1}
				\frac{\partial a^1} {\partial W} \text{(------ *note 3)}\\
				& = \bigg( - \frac{1}{N} \sum_{n=1}^{N} (y - z^{q+2})
					z^{q+2}(1-z^{q+2})
					V
					z^{q+1}(1-z^{q+1}) \bigg)
				z^1 (1-z^1)
				\frac{\partial a^1} {\partial W} \\
				& = \bigg( - \frac{1}{N} \sum_{n=1}^{N} (y - z^{q+2})
					z^{q+2}(1-z^{q+2})
					V
					z^{q+1}(1-z^{q+1}) \bigg)
				z^1 (1-z^1)
				\sum_{k,l} x_{i+k,j+l} \\
\end{split}
\end{equation*}

Therefore, the update equation for W is
\begin{equation*}
W^{t+1} = W^t - \eta \frac{\partial R}{\partial W}
\end{equation*}

Here's detailed explanations for each note.

note 1: The term (da2/dz1) is 1 if this point survived in the last down-sampling layer and 0 if not. We are only considering points which survived, so it's 1.

note 2: Since the down-samplying layers don't have weight, the gradient is
propogated without change. So we can use the gradient from q+1
layer. 

note 3: We expand dR/da(q+1) from the previous derivative. No surprise.

\end{enumerate}


\bigskip
References for the problem 3.

[1]: https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/

[2]: http://cs231n.github.io/convolutional-networks/
\end{document}
